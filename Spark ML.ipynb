{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "Cũng giống như các thuật toán phân loại Spark MLlib yêu cầu RDD của Đối tượng LabeledPoint, thuật toán Spark ML yêu cầu DataFrame of Row các đối tượng, bao gồm cả nhãn và tính năng. \n",
    "\n",
    "Cột nhãn chỉ định phân loại cho quan sát và cột tính năng chứa một SparseVector hoặc một đối tượng DenseVector. Một DenseVector được sử dụng khi mỗi quan sát chứa các tính năng giống nhau, trong khi một SparseVector được sử dụng khi các tính năng có thể thay đổi theo từng trường hợp — nghĩa là, một số tính năng có thể null hoặc không được điền cho một số trường hợp nhất định.\n",
    "\n",
    "Ưu điểm chính của SparseVector là nó chỉ lưu trữ các tính năng có giá trị, yêu cầu ít dung lượng hơn trong tập dữ liệu có chứa giá trị rỗng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import Row # Prepare DataFrame of labeled observations\n",
    "outlook = {\"sunny\": 0.0, \"overcast\": 1.0, \"rainy\": 2.0}\n",
    "observations = [\n",
    "Row(label=0, features=DenseVector([outlook[\"sunny\"],85,85,False])),\n",
    "Row(label=0, features=DenseVector([outlook[\"sunny\"],80,90,True])),\n",
    "Row(label=1, features=DenseVector([outlook[\"overcast\"],83,86,False])),\n",
    "Row(label=1, features=DenseVector([outlook[\"rainy\"],70,96,False])),\n",
    "Row(label=1, features=DenseVector([outlook[\"rainy\"],68,80,False])),\n",
    "Row(label=0, features=DenseVector([outlook[\"rainy\"],65,70,True])),\n",
    "Row(label=1, features=DenseVector([outlook[\"overcast\"],64,65,True])),\n",
    "Row(label=0, features=DenseVector([outlook[\"sunny\"],72,95,False])),\n",
    "Row(label=1, features=DenseVector([outlook[\"sunny\"],69,70,False])),\n",
    "Row(label=1, features=DenseVector([outlook[\"sunny\"],75,80,False])),\n",
    "Row(label=1, features=DenseVector([outlook[\"sunny\"],75,70,True])),\n",
    "Row(label=1, features=DenseVector([outlook[\"overcast\"],72,90,True])),\n",
    "Row(label=1, features=DenseVector([outlook[\"overcast\"],81,75,False])),\n",
    "Row(label=0, features=DenseVector([outlook[\"rainy\"],71,91,True]))\n",
    "]\n",
    "rdd = sc.parallelize(observations)\n",
    "data = spark.createDataFrame(rdd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|label|           features|\n",
      "+-----+-------------------+\n",
      "|    0|[0.0,85.0,85.0,0.0]|\n",
      "|    0|[0.0,80.0,90.0,1.0]|\n",
      "|    1|[1.0,83.0,86.0,0.0]|\n",
      "|    1|[2.0,70.0,96.0,0.0]|\n",
      "|    1|[2.0,68.0,80.0,0.0]|\n",
      "|    0|[2.0,65.0,70.0,1.0]|\n",
      "|    1|[1.0,64.0,65.0,1.0]|\n",
      "|    0|[0.0,72.0,95.0,0.0]|\n",
      "|    1|[0.0,69.0,70.0,0.0]|\n",
      "|    1|[0.0,75.0,80.0,0.0]|\n",
      "|    1|[0.0,75.0,70.0,1.0]|\n",
      "|    1|[1.0,72.0,90.0,1.0]|\n",
      "|    1|[1.0,81.0,75.0,0.0]|\n",
      "|    0|[2.0,71.0,91.0,1.0]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dữ liệu thành các tập training và thử ngiệm\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3]) # Train decision tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "model = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|label|           features|\n",
      "+-----+-------------------+\n",
      "|    0|[0.0,85.0,85.0,0.0]|\n",
      "|    1|[1.0,83.0,86.0,0.0]|\n",
      "|    0|[2.0,65.0,70.0,1.0]|\n",
      "|    1|[2.0,70.0,96.0,0.0]|\n",
      "|    0|[0.0,72.0,95.0,0.0]|\n",
      "|    1|[0.0,69.0,70.0,0.0]|\n",
      "|    1|[1.0,64.0,65.0,1.0]|\n",
      "|    1|[0.0,75.0,70.0,1.0]|\n",
      "|    1|[0.0,75.0,80.0,0.0]|\n",
      "|    1|[1.0,72.0,90.0,1.0]|\n",
      "|    1|[1.0,81.0,75.0,0.0]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|label|           features|\n",
      "+-----+-------------------+\n",
      "|    0|[0.0,80.0,90.0,1.0]|\n",
      "|    1|[2.0,68.0,80.0,0.0]|\n",
      "|    0|[2.0,71.0,91.0,1.0]|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_2a70b5c7b84a, depth=5, numNodes=11, numClasses=2, numFeatures=4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------+-----------+----------+\n",
      "|label|           features|rawPrediction|probability|prediction|\n",
      "+-----+-------------------+-------------+-----------+----------+\n",
      "|    0|[0.0,80.0,90.0,1.0]|    [0.0,4.0]|  [0.0,1.0]|       1.0|\n",
      "|    1|[2.0,68.0,80.0,0.0]|    [0.0,2.0]|  [0.0,1.0]|       1.0|\n",
      "|    0|[2.0,71.0,91.0,1.0]|    [0.0,2.0]|  [0.0,1.0]|       1.0|\n",
      "+-----+-------------------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Đưa ra dự đoán bằng cách sử dụng tập dữ liệu thử nghiệm\n",
    "predictions = model.transform(testData)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.666667 \n"
     ]
    }
   ],
   "source": [
    "# Đánh giá độ chính xác của mô hình\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Using Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row \n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data, split data into training and test datasets\n",
    "data = spark.sparkContext.textFile('data/movielens.dat')\n",
    "ratings_rdd = data.map(lambda x: x.split('\\t')) \\\n",
    " .map(lambda x: Row(userId = int(x[0]), movieId=int(x[1]), rating=float(x[2]), timestamp=int(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(userId=196, movieId=242, rating=3.0, timestamp=881250949)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|   196|    242|   3.0|881250949|\n",
      "|   186|    302|   3.0|891717742|\n",
      "|    22|    377|   1.0|878887116|\n",
      "|   244|     51|   2.0|880606923|\n",
      "|   166|    346|   1.0|886397596|\n",
      "|   298|    474|   4.0|884182806|\n",
      "|   115|    265|   2.0|881171488|\n",
      "|   253|    465|   5.0|891628467|\n",
      "|   305|    451|   3.0|886324817|\n",
      "|     6|     86|   3.0|883603013|\n",
      "|    62|    257|   2.0|879372434|\n",
      "|   286|   1014|   5.0|879781125|\n",
      "|   200|    222|   5.0|876042340|\n",
      "|   210|     40|   3.0|891035994|\n",
      "|   224|     29|   3.0|888104457|\n",
      "|   303|    785|   3.0|879485318|\n",
      "|   122|    387|   5.0|879270459|\n",
      "|   194|    274|   2.0|879539794|\n",
      "|   291|   1042|   4.0|874834944|\n",
      "|   234|   1184|   2.0|892079237|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.createDataFrame(ratings_rdd)\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=1, rating=5.0, timestamp=874965758),\n",
       " Row(userId=1, movieId=9, rating=5.0, timestamp=878543541),\n",
       " Row(userId=1, movieId=22, rating=4.0, timestamp=875072404),\n",
       " Row(userId=1, movieId=23, rating=4.0, timestamp=875072895),\n",
       " Row(userId=1, movieId=36, rating=2.0, timestamp=875073180)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "(training, test) = ratings.randomSplit([0.7, 0.3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=2, rating=3.0, timestamp=876893171),\n",
       " Row(userId=1, movieId=5, rating=3.0, timestamp=889751712),\n",
       " Row(userId=1, movieId=6, rating=5.0, timestamp=887431973),\n",
       " Row(userId=1, movieId=8, rating=1.0, timestamp=875072484),\n",
       " Row(userId=1, movieId=10, rating=3.0, timestamp=875693118)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=1, rating=5.0, timestamp=874965758),\n",
       " Row(userId=1, movieId=9, rating=5.0, timestamp=878543541),\n",
       " Row(userId=1, movieId=22, rating=4.0, timestamp=875072404),\n",
       " Row(userId=1, movieId=23, rating=4.0, timestamp=875072895),\n",
       " Row(userId=1, movieId=36, rating=2.0, timestamp=875073180)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALSModel: uid=ALS_f5be48135097, rank=10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training) # evaluate model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=251, movieId=148, rating=2.0, timestamp=886272547, prediction=3.593282461166382),\n",
       " Row(userId=633, movieId=148, rating=1.0, timestamp=875326138, prediction=3.942725419998169),\n",
       " Row(userId=406, movieId=148, rating=3.0, timestamp=879540276, prediction=2.1272995471954346),\n",
       " Row(userId=26, movieId=148, rating=3.0, timestamp=891377540, prediction=2.504910945892334),\n",
       " Row(userId=27, movieId=148, rating=3.0, timestamp=891543129, prediction=2.211848258972168)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3725028f7431>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Root-mean-square error = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'DataFrame'"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[{1311, 10.595923...|\n",
      "|   463|[{961, 6.297265},...|\n",
      "|   833|[{1085, 5.700978}...|\n",
      "+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# movie recommendations for each user\n",
    "model.recommendForAllUsers(3).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|   1580|[{153, 2.1682522}...|\n",
      "|    471|[{88, 6.0313516},...|\n",
      "|   1591|[{575, 11.41467},...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user recommendations for each movie\n",
    "model.recommendForAllItems(3).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Using Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means Clustering with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|           (3,[],[])|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_ca36e93c9116"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a k-means model\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeansModel: uid=KMeans_ca36e93c9116, k=2, distanceMeasure=euclidean, numFeatures=3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = kmeans.fit(dataset)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeansModel' object has no attribute 'computeCost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cf3f0ba6ac43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate using Within Set Sum of Squared Errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwssse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Within Set Sum of Squared Errors = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwssse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# returns:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Within Set Sum of Squared Errors = 0.11999999999994547\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KMeansModel' object has no attribute 'computeCost'"
     ]
    }
   ],
   "source": [
    "# evaluate using Within Set Sum of Squared Errors\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "# returns:\n",
    "# Within Set Sum of Squared Errors = 0.11999999999994547\n",
    "# show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[9.1 9.1 9.1]\n",
      "[0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    " print(center)\n",
    "# returns:\n",
    "# [ 0.1 0.1 0.1]\n",
    "# [ 9.1 9.1 9.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    " (0, \"a b c d e spark\", 1.0),\n",
    " (1, \"b d\", 0.0),\n",
    " (2, \"spark f g h\", 1.0),\n",
    " (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of 3 stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_c00632044f0c"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_597fd6e56eaa"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training) # Make predictions on test documents ...\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
