{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map()\n",
    "Syntax: RDD.map(<function>, preservesPartitioning=False).\n",
    "\n",
    "Trả về 1 RDD mới bằng cách truyền mỗi phần tử đầu vào (nguồn) qua hàm func."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap()\n",
    "Syntax: RDD.flatMap(<function>, preservesPartitioning=False).\n",
    "\n",
    "Tương tự map() nhưng khác map() ở chỗ, mỗi phần tử đầu vào qua flatMap sẽ trả về 0 hoặc nhiều phần tử đầu ra (có thể hiểu qua map sẽ là 1-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter()\n",
    "Syntax: RDD.filter(<function>).\n",
    "\n",
    "Trả về 1 RDD mới bằng cách chọn những phần tử đầu vào (nguồn) mà hàm func  trả về kết quả true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'MIT', 'License', '(MIT)', '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licenses = sc.textFile('opt/spark/licenses')\n",
    "words = licenses.flatMap(lambda x: x.split(' '))\n",
    "words.take(5)\n",
    "# returns [u'The', u'MIT', u'License', u'(MIT)', u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'mit', 'license', '(mit)', '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase = words.map(lambda x: x.lower())\n",
    "lowercase.take(5)\n",
    "# returns [u'the', u'mit', u'license', u'(mit)', u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['documentation',\n",
       " 'merchantability,',\n",
       " 'noninfringement.',\n",
       " 'redistribution',\n",
       " 'modification,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longwords = lowercase.filter(lambda x: len(x) > 12)\n",
    "longwords.take(5)\n",
    "# returns [u'documentation', u'merchantability,']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distinct()\n",
    "Syntax: RDD.distinct(numPartitions=None).\n",
    "\n",
    "Trả về 1 RDD mới chứa mỗi phần tử là duy nhất của tập dữ liệu nguồn (đầu vào)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 32015, Distinct Words: 2529\n"
     ]
    }
   ],
   "source": [
    "allwords = lowercase.count()\n",
    "distinctwords = lowercase.distinct().count()\n",
    "print (\"Total words: %s, Distinct Words: %s\" % (allwords, distinctwords))\n",
    "# returns \"Total words: 32015, Distinct Words: 2529\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupBy()\n",
    "Syntax: RDD.groupBy(<function>, numPartitions=None).\n",
    "    \n",
    "Trả về 1 RDD của các phần tử được nhóm bởi một func xác định."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'MIT', 'License', '(MIT)', 'Copyright']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "licenses = sc.textFile('opt/spark/licenses')\n",
    "words = licenses.flatMap(lambda x: x.split(' ')) \\\n",
    "                .filter(lambda x: len(x) > 0)\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[', <pyspark.resultiterable.ResultIterable at 0x220ea026580>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedbyfirstletter = words.groupBy(lambda x: x[0].lower())\n",
    "groupedbyfirstletter.take(1)\n",
    "# returns:\n",
    "# [('l', <pyspark.resultiterable.ResultIterable object at 0x7f678e9cca20>)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortBy()\n",
    "Syntax: RDD.sortBy(keyfunc , ascending=True, numPartitions=None).\n",
    "\n",
    "Sắp xếp một RDD theo đối số keyfunc (hàm được đặt tên hoặc ẩn danh) đề cử khóa cho một tập dữ liệu nhất định. Nó sắp xếp theo thứ tự sắp xếp của loại đối tượng chính. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You', 'you', 'you', 'you', 'You']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme = sc.textFile('opt/spark/README.md')\n",
    "words = readme.flatMap(lambda x: x.split(' ')) \\\n",
    "              .filter(lambda x: len(x) > 0)\n",
    "sortbyfirstletter = words.sortBy(lambda x: x[0].lower(), ascending=False)\n",
    "sortbyfirstletter.take(5)\n",
    "# returns ['You', 'you', 'you', 'you', 'you']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count()\n",
    "Syntax:RDD.count()\n",
    "\n",
    "Trả về số phần tử của tập dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()\n",
    "# returns 495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# collect()\n",
    "Syntax: RDD.collect()\n",
    "\n",
    "Trả về tất cả các phần tử của tập dữ liệu như 1 mảng ở driver Program. Hàm này hữu ích sau khi lọc hoặc thao tác khác mà trả về tập dữ liệu con đủ nhỏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'large-scale',\n",
       " 'data',\n",
       " 'processing.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Structured',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '<https://spark.apache.org/>',\n",
       " '[![Jenkins',\n",
       " 'Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       " '[![AppVeyor',\n",
       " 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark',\n",
       " 'Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " 'the',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](https://spark.apache.org/documentation.html).',\n",
       " 'This',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](https://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " './build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '(You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'if',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'a',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'is',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'general',\n",
       " 'development',\n",
       " 'tips,',\n",
       " 'including',\n",
       " 'info',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'an',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[\"Useful',\n",
       " 'Developer',\n",
       " 'Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " 'The',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'through',\n",
       " 'the',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " './bin/spark-shell',\n",
       " 'Try',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " 'scala>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " 'Alternatively,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " './bin/pyspark',\n",
       " 'And',\n",
       " 'run',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '>>>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'will',\n",
       " 'run',\n",
       " 'the',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'the',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'a',\n",
       " 'cluster.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mesos://',\n",
       " 'or',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " 'or',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'if',\n",
       " 'the',\n",
       " 'class',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'if',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'be',\n",
       " 'run',\n",
       " 'using:',\n",
       " './dev/run-tests',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'a',\n",
       " 'module,',\n",
       " 'or',\n",
       " 'individual',\n",
       " 'tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " 'There',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'Kubernetes',\n",
       " 'integration',\n",
       " 'test,',\n",
       " 'see',\n",
       " 'resource-managers/kubernetes/integration-tests/README.md',\n",
       " '##',\n",
       " 'A',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'the',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'the',\n",
       " 'same',\n",
       " 'version',\n",
       " 'that',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'Version',\n",
       " 'and',\n",
       " 'Enabling',\n",
       " 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '##',\n",
       " 'Configuration',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '[Configuration',\n",
       " 'Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'the',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.',\n",
       " '##',\n",
       " 'Contributing',\n",
       " 'Please',\n",
       " 'review',\n",
       " 'the',\n",
       " '[Contribution',\n",
       " 'to',\n",
       " 'Spark',\n",
       " 'guide](https://spark.apache.org/contributing.html)',\n",
       " 'for',\n",
       " 'information',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'contributing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'project.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take()\n",
    "Syntax: RDD.take(n)\n",
    "\n",
    "Trả về mảng gồm n phần tử đầu tiên của tập dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#', 'Apache', 'Spark']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top()\n",
    "Syntax: RDD.top(n, key=None)\n",
    "\n",
    "Trả về n phần tử hàng đầu từ RDD, nhưng không giống như với take (), với top() các phần tử được sắp xếp và trả về theo thứ tự giảm dần."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'you', 'with']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().top(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first()\n",
    "Syntax: RDD.first()\n",
    "\n",
    "Trả về phần tử đầu tiên của tập dữ liệu (tương tự take(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduce()\n",
    "Syntax: RDD.reduce('function')\n",
    "    \n",
    "Tổng hợp các phần tử của tập dữ liệu sử dụng hàm func (có 2 đối số và trả về 1 kết quả)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "numbers.reduce(lambda x, y: x + y) \n",
    "# returns 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fold()\n",
    "Syntax: RDD.fold(zeroValue, <'function'>)\n",
    "    \n",
    "Tổng hợp các phần tử của mỗi phân vùng của RDD và sau đó thực hiện phép toán tổng hợp dựa trên kết quả cho tất cả, bằng cách sử dụng chức năng và một zeroValue. Mặc dù reduce() và fold() tương tự nhau về chức năng, chúng khác nhau ở chỗ fold() không giao hoán và giá trị cuối cùng (zeroValue) là bắt buộc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "numbers.fold(0, lambda x, y: x + y)\n",
    "# returns 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can not reduce() empty RDD",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3d4bf5ceef51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mempty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mempty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# returns:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ValueError: Cannot reduce() empty RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1000\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not reduce() empty RDD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtreeReduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can not reduce() empty RDD"
     ]
    }
   ],
   "source": [
    "empty = sc.parallelize([])\n",
    "empty.reduce(lambda x, y: x + y)\n",
    "# returns:\n",
    "# ValueError: Cannot reduce() empty RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty.fold(0, lambda x, y: x + y)\n",
    "# returns 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foreach()\n",
    "Syntax: RDD.foreach(<'function'>)\n",
    "\n",
    "Chạy hàm func cho mỗi phần tử của tập dữ liệu. Điều này có tác dụng khi thực hiện cập nhật 1 biến accumulator hoặc tương tác với hệ thống lưu trữ ngoài."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printfunc(x): print(x)\n",
    "licenses = sc.textFile('opt/spark/licenses')\n",
    "longwords = licenses.flatMap(lambda x: x.split(' ')) \\\n",
    "                    .filter(lambda x: len(x) > 12)\n",
    "longwords.foreach(lambda x: printfunc(x))\n",
    "# returns:\n",
    "# ...\n",
    "# Redistributions\n",
    "# documentation\n",
    "# distribution.\n",
    "# MERCHANTABILITY\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations on PairRDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keys()\n",
    "Syntax: RDD.keys()\n",
    "\n",
    "Trả về một RDD với các khóa từ một cặp key/value RDD hoặc phần tử đầu tiên từ mỗi bộ trong một cặp key/value RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'state', 'zip', 'country']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvpairs = sc.parallelize([('city','Hayward'),\n",
    "                          ('state','CA'),\n",
    "                          ('zip',94541),\n",
    "                          ('country','USA')])\n",
    "kvpairs.keys().collect()\n",
    "# returns ['city', 'state', 'zip', 'country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# values()\n",
    "Syntax: RDD.values()\n",
    "\n",
    "Trả về một RDD với các giá trị từ một cặp key/value RDD hoặc phần tử thứ hai từ mỗi bộ trong một cặp key/value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hayward', 'CA', 94541, 'USA']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvpairs = sc.parallelize([('city','Hayward'),\n",
    "                          ('state','CA'),\n",
    "                          ('zip',94541),\n",
    "                          ('country','USA')])\n",
    "kvpairs.values().collect()\n",
    "# returns ['Hayward', 'CA', 94541, 'USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyBy()\n",
    "Syntax: RDD.keyBy(<'func'>)\n",
    "\n",
    "Tạo ra một bộ giá trị bao gồm một khóa và một giá trị từ các phần tử trong RDD bằng cách áp dụng một chức năng được chỉ định bởi đối số func. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ('Hayward', 'USA', 1)),\n",
       " (2, ('Baumholder', 'Germany', 2)),\n",
       " (3, ('Alexandria', 'USA', 3)),\n",
       " (4, ('Melbourne', 'Australia', 4))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = sc.parallelize([('Hayward', 'USA', 1),\n",
    "                            ('Baumholder','Germany', 2),\n",
    "                            ('Alexandria','USA', 3),\n",
    "                            ('Melbourne','Australia', 4)])\n",
    "bylocno = locations.keyBy(lambda x: x[2])\n",
    "bylocno.collect()\n",
    "# returns:\n",
    "#[(1, ('Hayward', 'USA', 1)), (2, ('Baumholder', 'Germany', 2)),\n",
    "# (3, ('Alexandria', 'USA', 3)), (4, ('Melbourne', 'Australia', 4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapValues()\n",
    "Syntax: RDD.mapValues(<'function'>)\n",
    "\n",
    "Chuyển từng giá trị trong một cặp  key/value RDD thông qua một chức năng (một chức năng được đặt tên hoặc ẩn danh được chỉ định bởi đối số <'function'>) mà không cần thay đổi các key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMapValues()\n",
    "Syntax: RDD.flatMapValues(<'function'>)\n",
    "\n",
    "Chuyển từng giá trị trong một cặp key/value RDD thông qua một chức năng mà không cần thay đổi các key và tạo ra một danh sách phẳng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hayward', '71|69|71|71|72'],\n",
       " ['Baumholder', '46|42|40|37|39'],\n",
       " ['Alexandria', '50|48|51|53|44'],\n",
       " ['Melbourne', '88|101|85|77|74']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locwtemps = sc.parallelize(['Hayward,71|69|71|71|72',\n",
    "                            'Baumholder,46|42|40|37|39',\n",
    "                            'Alexandria,50|48|51|53|44',\n",
    "                            'Melbourne,88|101|85|77|74'])\n",
    "kvpairs = locwtemps.map(lambda x: x.split(','))\n",
    "kvpairs.collect()\n",
    "# returns :\n",
    "# [['Hayward', '71|69|71|71|72'],\n",
    "# ['Baumholder', '46|42|40|37|39'],\n",
    "# ['Alexandria', '50|48|51|53|44'],\n",
    "# ['Melbourne', '88|101|85|77|74']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', [71, 69, 71, 71, 72]),\n",
       " ('Baumholder', [46, 42, 40, 37, 39]),\n",
       " ('Alexandria', [50, 48, 51, 53, 44]),\n",
       " ('Melbourne', [88, 101, 85, 77, 74])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locwtemplist = kvpairs.mapValues(lambda x: x.split('|')) \\\n",
    "                      .mapValues(lambda x: [int(s) for s in x])\n",
    "locwtemplist.collect()\n",
    "# returns :\n",
    "# [('Hayward', [71, 69, 71, 71, 72]),\n",
    "# ('Baumholder', [46, 42, 40, 37, 39]),\n",
    "# ('Alexandria', [50, 48, 51, 53, 44]),\n",
    "# ('Melbourne', [88, 101, 85, 77, 74])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 71),\n",
       " ('Hayward', 69),\n",
       " ('Hayward', 71),\n",
       " ('Hayward', 71),\n",
       " ('Hayward', 72),\n",
       " ('Baumholder', 46),\n",
       " ('Baumholder', 42),\n",
       " ('Baumholder', 40),\n",
       " ('Baumholder', 37),\n",
       " ('Baumholder', 39),\n",
       " ('Alexandria', 50),\n",
       " ('Alexandria', 48),\n",
       " ('Alexandria', 51),\n",
       " ('Alexandria', 53),\n",
       " ('Alexandria', 44),\n",
       " ('Melbourne', 88),\n",
       " ('Melbourne', 101),\n",
       " ('Melbourne', 85),\n",
       " ('Melbourne', 77),\n",
       " ('Melbourne', 74)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locwtemps = kvpairs.flatMapValues(lambda x: x.split('|')) \\\n",
    "                   .map(lambda x: (x[0], int(x[1])))\n",
    "locwtemps.collect()\n",
    "# returns :\n",
    "# [('Hayward', 71), ('Hayward', 69), ('Hayward', 71)]..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupByKey()\n",
    "Syntax: RDD.groupByKey(numPartitions=None, partitionFunc= <'hash_fn>)\n",
    "\n",
    "Khi gọi đến 1 tập dữ liệu (K,V) sẽ trả về 1 tập là cặp (K,Seq(V))(Tức là nhóm tập các phần tử cùng Key). \n",
    "Chú ý: mặc định chỉ có 8 task song song khi grouping. Có thể thay đổi số task song song này bằng việc truyền vào tham số đầu vào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', <pyspark.resultiterable.ResultIterable at 0x220ec79df70>)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = locwtemps.groupByKey()\n",
    "grouped.take(1)\n",
    "# returns:\n",
    "# [('Melbourne', <pyspark.resultiterable.ResultIterable object at 0x7f121ce11390>)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 70.8),\n",
       " ('Baumholder', 40.8),\n",
       " ('Melbourne', 85.0),\n",
       " ('Alexandria', 49.2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgtemps = grouped.mapValues(lambda x: sum(x)/len(x))\n",
    "avgtemps.collect()\n",
    "# returns:\n",
    "# [('Melbourne', 85), ('Baumholder', 40), ('Alexandria', 49), ('Hayward', 70)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduceByKey()\n",
    "Syntax: RDD.reduceByKey(<'function'>, numPartitions=None, partitionFunc=<'hash_fn'>)\n",
    "    \n",
    "Khi gọi tập dữ liệu (K,V), trả về 1 tập (K,V) mà giá trị của key được tổng hợp sử dụng hàm reduce func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 70.8),\n",
       " ('Baumholder', 40.8),\n",
       " ('Melbourne', 85.0),\n",
       " ('Alexandria', 49.2)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temptups = locwtemps.mapValues(lambda x: (x, 1))\n",
    "# creates tuples (city, (temp, 1))\n",
    "inputstoavg = temptups.reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1]))\n",
    "# sums temperatures by city\n",
    "averages = inputstoavg.map(lambda x: (x[0], x[1][0]/x[1][1]))\n",
    "# divides the sum of temperatures by key by the number of readings\n",
    "averages.collect()\n",
    "# returns :\n",
    "# [('Baumholder', 40.8),\n",
    "# ('Melbourne', 85.0),\n",
    "# ('Alexandria', 49.2),\n",
    "# ('Hayward', 70.8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# foldByKey()\n",
    "Syntax: RDD.foldByKey(zeroValue, <'function'>, numPartitions=None, partitionFunc=<'hash_fn'>)\n",
    "\n",
    "Tương tự như hàm fold(). Tuy nhiên, foldByKey() là mộtchuyển đổi hoạt động với các phần tử key/value được xác định trước. Cả foldByKey() và fold() đều cung cấp đối số zeroValue của cùng loại sẽ được sử dụng nếu RDD trống."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hayward', 72), ('Baumholder', 46), ('Melbourne', 101), ('Alexandria', 53)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxbycity = locwtemps.foldByKey(0, lambda x, y: x if x > y else y)\n",
    "maxbycity.collect()\n",
    "# returns :\n",
    "# [('Baumholder', 46), ('Melbourne', 101), ('Alexandria', 53), ('Hayward', 72)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sortByKey()\n",
    "Syntax:RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=<'function'>)\n",
    "\n",
    "Khi gọi tập dữ liệu (K,V) với K có thể thực hiện sắp thứ tự được. Khi đó, nó sẽ trả về tập dữ liệu (K,V) được sắp sếp tăng dần hoặc giảm dần theo key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alexandria', 50),\n",
       " ('Alexandria', 48),\n",
       " ('Alexandria', 51),\n",
       " ('Alexandria', 53)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedbykey = locwtemps.sortByKey()\n",
    "sortedbykey.take(4)\n",
    "# returns:\n",
    "# [('Alexandria', 50), ('Alexandria', 48), ('Alexandria', 51), ('Alexandria', 53)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 'Melbourne'), (88, 'Melbourne'), (85, 'Melbourne'), (77, 'Melbourne')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedbyval = locwtemps.map(lambda x: (x[1],x[0])) \\\n",
    "                       .sortByKey(ascending=False)\n",
    "sortedbyval.take(4)\n",
    "# returns:# [(101, 'Melbourne'), (88, 'Melbourne'), (85, 'Melbourne'), (77, 'Melbourne')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
